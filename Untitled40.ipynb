{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled40.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPPchuk1oGOdyLGepOy/Cyq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LeeJeongWoon-Workout/Deep_Learning_Pytorch/blob/main/Untitled40.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAwP6c18Bn45",
        "outputId": "32f374c6-4e71-41a7-fc9d-bb276c323c83"
      },
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as pltxxxxx\n",
        "%matplotlib inline\n",
        "\n",
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets,transforms                 \n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torchvision\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D__G4oO6Bn_U"
      },
      "source": [
        "import torch\n",
        "import torch.nn as  nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(out_channels, out_channels*self.expansion, kernel_size=1, stride=1, padding=0)\n",
        "        self.batch_norm3 = nn.BatchNorm2d(out_channels*self.expansion)\n",
        "        \n",
        "        self.i_downsample = i_downsample\n",
        "        self.stride = stride\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        identity = x.clone()\n",
        "        x = self.relu(self.batch_norm1(self.conv1(x)))\n",
        "        \n",
        "        x = self.relu(self.batch_norm2(self.conv2(x)))\n",
        "        \n",
        "        x = self.conv3(x)\n",
        "        x = self.batch_norm3(x)\n",
        "        \n",
        "        #downsample if needed\n",
        "        if self.i_downsample is not None:\n",
        "            identity = self.i_downsample(identity)\n",
        "        #add identity\n",
        "        x+=identity\n",
        "        x=self.relu(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "    expansion = 1\n",
        "    def __init__(self, in_channels, out_channels, i_downsample=None, stride=1):\n",
        "        super(Block, self).__init__()\n",
        "       \n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=stride, bias=False)\n",
        "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.i_downsample = i_downsample\n",
        "        self.stride = stride\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "      identity = x.clone()\n",
        "\n",
        "      x = self.relu(self.batch_norm2(self.conv1(x)))\n",
        "      x = self.batch_norm2(self.conv2(x))\n",
        "\n",
        "      if self.i_downsample is not None:\n",
        "          identity = self.i_downsample(identity)\n",
        "      print(x.shape)\n",
        "      print(identity.shape)\n",
        "      x += identity\n",
        "      x = self.relu(x)\n",
        "      return x\n",
        "\n",
        "\n",
        "        \n",
        "        \n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, ResBlock, layer_list, num_classes, num_channels=3):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_channels = 64\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.batch_norm1 = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size = 3, stride=2, padding=1)\n",
        "        \n",
        "        self.layer1 = self._make_layer(ResBlock, layer_list[0], planes=64)\n",
        "        self.layer2 = self._make_layer(ResBlock, layer_list[1], planes=128, stride=2)\n",
        "        self.layer3 = self._make_layer(ResBlock, layer_list[2], planes=256, stride=2)\n",
        "        self.layer4 = self._make_layer(ResBlock, layer_list[3], planes=512, stride=2)\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
        "        self.fc = nn.Linear(512*ResBlock.expansion, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.batch_norm1(self.conv1(x)))\n",
        "        x = self.max_pool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        \n",
        "        x = self.avgpool(x)\n",
        "        x = x.reshape(x.shape[0], -1)\n",
        "        x = self.fc(x)\n",
        "        \n",
        "        return x\n",
        "        \n",
        "    def _make_layer(self, ResBlock, blocks, planes, stride=1):\n",
        "        ii_downsample = None\n",
        "        layers = []\n",
        "        \n",
        "        if stride != 1 or self.in_channels != planes*ResBlock.expansion:\n",
        "            ii_downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, planes*ResBlock.expansion, kernel_size=1, stride=stride),\n",
        "                nn.BatchNorm2d(planes*ResBlock.expansion)\n",
        "            )\n",
        "            \n",
        "        layers.append(ResBlock(self.in_channels, planes, i_downsample=ii_downsample, stride=stride))\n",
        "        self.in_channels = planes*ResBlock.expansion\n",
        "        \n",
        "        for i in range(blocks-1):\n",
        "            layers.append(ResBlock(self.in_channels, planes))\n",
        "            \n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "        \n",
        "        \n",
        "def ResNet50(num_classes=90, channels=3):\n",
        "    return ResNet(Bottleneck, [3,4,6,3], num_classes, channels)\n",
        "    \n",
        "def ResNet101(num_classes, channels=3):\n",
        "    return ResNet(Bottleneck, [3,4,23,3], num_classes, channels)\n",
        "\n",
        "def ResNet152(num_classes, channels=3):\n",
        "    return ResNet(Bottleneck, [3,8,36,3], num_classes, channels)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbxsDxtaJQRW"
      },
      "source": [
        "!pwd\n",
        "!unzip '/content/drive/MyDrive/Colab Notebooks/deeplearning/deeplearning2021competition.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "He4PhmUFJQTi"
      },
      "source": [
        "class AverageMeter(object):\n",
        "    r\"\"\"Computes and stores the average and current value\n",
        "    \"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, *meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def print(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    r\"\"\"Computes the accuracy over the $k$ top predictions for the specified values of k\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        # _, pred = output.topk(maxk, 1, True, True)\n",
        "        # pred = pred.t()\n",
        "        # correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        # faster topk (ref: https://github.com/pytorch/pytorch/issues/22812)\n",
        "        _, idx = output.sort(descending=True)\n",
        "        pred = idx[:,:maxk]\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res\n",
        "from typing import Any, Callable, cast, Dict, List, Optional, Tuple\n",
        "from PIL import Image\n",
        "\n",
        "IMG_EXTENSIONS = (\".jpg\", \".jpeg\", \".png\", \".ppm\", \".bmp\", \".pgm\", \".tif\", \".tiff\", \".webp\")\n",
        "\n",
        "from PIL import Image\n",
        "def pil_loader(path: str) -> Image.Image:\n",
        "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "    with open(path, \"rb\") as f:\n",
        "        img = Image.open(f)\n",
        "        return img.convert(\"RGB\")\n",
        "\n",
        "def default_loader(path: str) -> Any:\n",
        "    from torchvision import get_image_backend\n",
        "\n",
        "    if get_image_backend() == \"accimage\":\n",
        "        return accimage_loader(path)\n",
        "    else:\n",
        "        return pil_loader(path)\n",
        "\n",
        "class FoodImageFolder(torchvision.datasets.DatasetFolder):\n",
        "    \"\"\"A generic data loader where the images are arranged in this way by default: ::\n",
        "        root/dog/xxx.png\n",
        "        root/dog/xxy.png\n",
        "        root/dog/[...]/xxz.png\n",
        "        root/cat/123.png\n",
        "        root/cat/nsdf3.png\n",
        "        root/cat/[...]/asd932_.png\n",
        "    This class inherits from :class:`~torchvision.datasets.DatasetFolder` so\n",
        "    the same methods can be overridden to customize the dataset.\n",
        "    Args:\n",
        "        root (string): Root directory path.\n",
        "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
        "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
        "        target_transform (callable, optional): A function/transform that takes in the\n",
        "            target and transforms it.\n",
        "        loader (callable, optional): A function to load an image given its path.\n",
        "        is_valid_file (callable, optional): A function that takes path of an Image file\n",
        "            and check if the file is a valid file (used to check of corrupt files)\n",
        "     Attributes:\n",
        "        classes (list): List of the class names sorted alphabetically.\n",
        "        class_to_idx (dict): Dict with items (class_name, class_index).\n",
        "        imgs (list): List of (image path, class_index) tuples\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        root: str,\n",
        "        transform: Optional[Callable] = None,\n",
        "        target_transform: Optional[Callable] = None,\n",
        "        loader: Callable[[str], Any] = default_loader,\n",
        "        is_valid_file: Optional[Callable[[str], bool]] = None,\n",
        "        txt_file=None\n",
        "    ):\n",
        "        super().__init__(\n",
        "            root,\n",
        "            loader,\n",
        "            IMG_EXTENSIONS if is_valid_file is None else None,\n",
        "            transform=transform,\n",
        "            target_transform=target_transform,\n",
        "            is_valid_file=is_valid_file,\n",
        "        )\n",
        "\n",
        "        if txt_file is not None:\n",
        "            f = open(txt_file, 'r')\n",
        "            id = 0\n",
        "\n",
        "            path_list = []\n",
        "            while True:\n",
        "                line = f.readline()[:-1]\n",
        "                if not line: break\n",
        "                path = root+line+\".jpg\"\n",
        "                path_list.append(path)\n",
        "\n",
        "            f.close()\n",
        "\n",
        "            self.imgs = []\n",
        "            for sample in self.samples:\n",
        "                if sample[0] in path_list:\n",
        "                    self.imgs.append(sample)\n",
        "            \n",
        "            self.samples = self.imgs\n",
        "        else:\n",
        "            self.imgs = self.samples\n",
        "\n",
        "        print(len(self.imgs))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYOe_a1WJQWC"
      },
      "source": [
        "\n",
        "                                      transforms.Lambda(lambda crops: torch.stack([transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])(crop) for crop in crops]))])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "id": "IOim-a-KMTcv",
        "outputId": "d3bc2f05-e432-4d8f-c4ac-8a91433573cd"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "model_name='resnet50'\n",
        "\n",
        "\n",
        "\n",
        "model = resnet50()\n",
        "\n",
        "SAVEPATH = '/content/drive/MyDrive/Colab Notebooks/deeplearning'\n",
        "# specify loss function (categorical cross-entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# specify optimizer (stochastic gradient descent with momentum)\n",
        "optimizer = optim.SGD(model.fc.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# secify learning rate scheduler (if there is no further decrease in loss for next 5 epochs \n",
        "# then lower the learning rate by 0.1)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 5)\n",
        "\n",
        "# Data augmentation\n",
        "train_transforms = transforms.Compose([transforms.RandomResizedCrop(224),\n",
        "                                       transforms.RandomHorizontalFlip(),\n",
        "                                       transforms.RandomVerticalFlip(),\n",
        "                                       transforms.RandomRotation(45),\n",
        "                                       transforms.RandomAffine(45),\n",
        "                                       transforms.ColorJitter(),\n",
        "                                       transforms.ToTensor(),\n",
        "                                       transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                            std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "# Use 10-crop for Test Time Augmentation\n",
        "test_transforms = transforms.Compose([transforms.Resize(256),\n",
        "                                      transforms.TenCrop(224),\n",
        "                                      transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops]))\n",
        "\n",
        "\n",
        "\n",
        "train_dataset = FoodImageFolder('./food_data/processed_images/', transform=train_transforms, txt_file='./food_data/meta/meta/train.txt')\n",
        "train_dataset,valid_dataset=torch.utils.data.random_split(train_dataset,[36782,2000])\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size,shuffle=True,num_workers=4,pin_memory=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size,shuffle=False,num_workers=4,pin_memory=True)\n",
        "\n",
        "#I made two data_loader from train dataset first is train_loader second is valid_loader. all of them is from train_dataset. I do not use test dataset\n",
        "\n",
        "criterion = criterion.cuda()\n",
        "\n",
        "cuda = True\n",
        "epochs = 100\n",
        "writer = SummaryWriter() # For Tensorboard\n",
        "early_stop_count=0\n",
        "ES_patience=10\n",
        "best = 0.0\n",
        "if cuda:\n",
        "    model.cuda()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    \n",
        "    # Training\n",
        "    model.train()\n",
        "    correct = 0\n",
        "    train_loss = 0.0\n",
        "    tbar = tqdm(train_loader, desc = 'Training', position=0, leave=True)\n",
        "    for i,(inp,lbl) in enumerate(tbar):\n",
        "        optimizer.zero_grad()\n",
        "        if cuda:\n",
        "            inp,lbl = inp.cuda(),lbl.cuda()\n",
        "        out = model(inp)\n",
        "        loss = criterion(out,lbl)\n",
        "        train_loss += loss\n",
        "        out = out.argmax(dim=1)\n",
        "        correct += (out == lbl).sum().item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        tbar.set_description(f\"Epoch: {epoch+1}, loss: {loss.item():.5f}, acc: {100.0*correct/((i+1)*train_loader.batch_size):.4f}%\")\n",
        "    train_acc = 100.0*correct/len(train_loader.dataset)\n",
        "    train_loss /= (len(train_loader.dataset)/batch_size)\n",
        "\n",
        "    #Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        val_loss = 0.0\n",
        "        vbar = tqdm(valid_loader, desc = 'Validation', position=0, leave=True)\n",
        "        for i,(inp,lbl) in enumerate(vbar):\n",
        "            if cuda:\n",
        "                inp,lbl = inp.cuda(),lbl.cuda()\n",
        "            out = model(inp)\n",
        "            val_loss += criterion(out,lbl)\n",
        "            out = out.argmax(dim=1)\n",
        "            correct += (out == lbl).sum().item()\n",
        "        val_acc = 100.0*correct/len(valid_loader.dataset)\n",
        "        val_loss /= (len(valid_loader.dataset)/batch_size)\n",
        "    print(f'\\nEpoch: {epoch+1}/{epochs}')\n",
        "    print(f'Train loss: {train_loss}, Train Accuracy: {train_acc}')\n",
        "    print(f'Validation loss: {val_loss}, Validation Accuracy: {val_acc}\\n')\n",
        "\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # write to tensorboard\n",
        "    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
        "    writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
        "    writer.add_scalar(\"Accuracy/train\", train_acc, epoch)\n",
        "    writer.add_scalar(\"Accuracy/val\", val_acc, epoch)\n",
        "\n",
        "    if val_acc>best:\n",
        "        best=val_acc\n",
        "        torch.save(model,model_name)\n",
        "        early_stop_count=0\n",
        "        print('Accuracy Improved, model saved.\\n')\n",
        "    else:\n",
        "        early_stop_count+=1\n",
        "\n",
        "    if early_stop_count==ES_patience:\n",
        "        print('Early Stopping Initiated...')\n",
        "        print(f'Best Accuracy achieved: {best:.2f}% at epoch:{epoch+1-ES_patience}')\n",
        "        print(f'Model saved as {model_name}')\n",
        "        break\n",
        "    writer.flush()\n",
        "# writer.close()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-10-edb9d315067d>\"\u001b[0;36m, line \u001b[0;32m45\u001b[0m\n\u001b[0;31m    train_dataset = FoodImageFolder('./food_data/processed_images/', transform=train_transforms, txt_file='./food_data/meta/meta/train.txt')\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l11DIPVrJQYX"
      },
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import time\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "def eval():\n",
        "    #test data augmentation\n",
        "    test_transform = torchvision.transforms.Compose([\n",
        "            torchvision.transforms.ToTensor(),\n",
        "            torchvision.transforms.Resize((224,224)),\n",
        "            torchvision.transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
        "    ])\n",
        "\n",
        "    test_dataset = FoodImageFolder(\n",
        "        './food_data/processed_images/', transform=test_transform, txt_file='./food_data/meta/meta/test.txt')\n",
        "    test_loader = DataLoader(test_dataset, batch_size=BATCHSIZE, num_workers=2, shuffle=False)\n",
        "\n",
        "    #I just add this code to use best validation accuracy model made by my training loop\n",
        "    model = torch.load('/content/resnet50')\n",
        "    model = model.cuda()\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    print('Make an evaluation csv file for kaggle submission...')\n",
        "    Category = []\n",
        "    i = 0\n",
        "    for input, _ in test_loader:\n",
        "        i+=1\n",
        "        input = input.cuda()\n",
        "        output = model(input)\n",
        "        output = torch.argmax(output, dim=1)\n",
        "        Category = Category + output.tolist()\n",
        "\n",
        "    Id = list(range(0, 20665))\n",
        "    samples = {\n",
        "       'Id': Id,\n",
        "       'Category': Category \n",
        "    }\n",
        "    df = pd.DataFrame(samples, columns=['Id', 'Category'])\n",
        "\n",
        "    df.to_csv(SAVEPATH+'submission.csv', index=False)\n",
        "    print('Done!!')\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YerLUT0JQaj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o75ikIstJQc6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVqOE7GdJQfA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxcgVExXJQhC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAYbLSJ_JQjH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Li5lK_UTJQlM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbXruwdZJQpa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}